{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright IBM Inc. All Rights Reserved.\n",
    "#### SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Submission and Control\n",
    "\n",
    "\n",
    "This notebook demonstrates\n",
    "* Authenticating to the `st4sd-runtime-service` REST-API\n",
    "* Querying available experiments\n",
    "* Submitting a [`band-gap-pm3-gamess-us`](https://github.com/st4sd/band-gap-gamess/) experiment (semi-emperical variant)\n",
    "* Querying status of experiment instance\n",
    "* Retrieving top-level output results from the instance\n",
    "\n",
    "For information on how to retrieve detailed information via the `st4sd-datastore` see the notebook `ST4SD Datastore - Common Query Examples` located in same repository as this notebook.\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Go to https://st4sd.github.io/overview/api-docs/openapi/st4sd-runtime-service/st4sd-runtime-service.openapi.html for the full API specification\n",
    "* For details on the **band-gap-gamess** experiments see https://github.com/st4sd/band-gap-gamess/\n",
    "   \n",
    "## Terminology\n",
    "- **experiment**: Refers to the *definition* of a particular workflow e.g. the `band-gap-pm3-gamess-us` experiment\n",
    "- **instance**: Refers to a particular execution of an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import experiment.service.db\n",
    "import json\n",
    "import urllib\n",
    "import logging\n",
    "import typing\n",
    "import json\n",
    "import typing\n",
    "import time\n",
    "import datetime\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(levelname)-9s %(name)-15s: %(funcName)-20s %(asctime)-15s: %(message)s')\n",
    "root=logging.getLogger()\n",
    "root.setLevel(logging.CRITICAL)\n",
    "\n",
    "def pretty_json(what):\n",
    "    return json.dumps(what, indent=2, sort_keys=True)\n",
    "\n",
    "# This is just a jupyter notebook helper class to display information\n",
    "class PrettyInstanceStatus:\n",
    "    def __init__(self, status: typing.Dict[str, typing.Any]):\n",
    "        self.text = pretty_json(status)\n",
    "    \n",
    "    def _repr_markdown_(self):\n",
    "        exp_state = instance_status['status']['experiment-state']\n",
    "        outputs = instance_status['outputs']\n",
    "        \n",
    "        ret = f\"\"\"\n",
    "Experiment state is: **{exp_state}** <br>\n",
    "Experiment has produced the following outputs so far {list(outputs)} <br> <br>\n",
    "\n",
    "```json\n",
    "{self.text}\n",
    "```\n",
    "\"\"\"\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the routes below to match the ones of your OpenShift cluster. \n",
    "\n",
    "**Note**: The ST4SD instance in this example requires that you use a TUNNELALL VPN. [Here](https://w3.ibm.com/#/support/article/cisco_anyconnect_vpn_client_troubleshooting_tips/cnas) you can find instructions to configure your TUNNELALL VPN.\n",
    "\n",
    "**Note**: You just need to define `route_st4sd_runtime_service` endpoint, the ExperimentRESTAPI wrapper will discover the remaining endpoints automatically.\n",
    "\n",
    "**Note**: You must be an OpenShift user with access to the namespace that hosts the ST4SD instance before you can authenticate to the ST4SD instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this route belongs to a cluster with minimal resources \n",
    "# e.g. can process roughly 1/2 mols at a time\n",
    "# NOTE: This cluster requires a TUNNELALL VPN\n",
    "route_st4sd_runtime_service = 'https://st4sd-prod.ve-5446-9ca4d14d48413d18ce61b80811ba4308-0000.us-south.containers.appdomain.cloud'\n",
    "route_st4sd_rest = None\n",
    "route_st4sd_registry = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate to the ST4SD Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtually all instances of the ST4SD stack have authentication enabled. Please follow the instructions in the cells below to correctly authenticate, if required.\n",
    "\n",
    "**NOTE: If you are running this notebook via the st4sd-runtime-core container please ensure it has been recently updated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authentication_enabled = False\n",
    "try:\n",
    "    response = urllib.request.urlopen('/'.join((route_st4sd_runtime_service, 'experiments/')))\n",
    "except HTTPError as e:\n",
    "    if e.code == 403:\n",
    "        authentication_enabled = True\n",
    "        print(\"Authentication is enabled, please proceed with the 'Authentication Enabled' section\")\n",
    "    else:\n",
    "        print(\"Authentication is not enabled, skip to 'Connect to API'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication Enabled\n",
    "- Visit the URL printed in the cell below\n",
    "- If it's your first time accessing this ST4SD instance you will need to login to the OpenShift cluster once before you can proceed further.\n",
    "   - Contact the administrator of the OpenShift cluster hosting the ST4SD instance. They need to add you as a user on their OpenShift cluster and give you permission to view the namespaced objects in the OpenShift project hosting the ST4SD instance.\n",
    "   - Upon visiting the ${url_sign_in} you will have to choose the login method (depending on the OpenShift instance this can be W3Id, IBM SSO, LDAP, etc)\n",
    "   - If this is the first time you login, you will be prompted to give your consent for the workflow-operator ServiceAccount to know that your username has authenticated to OpenShift. You need to agree to this before you can authenticate to the `st4sd-runtime-service` REST-API.\n",
    "- After logging in, you will be presented with an authentication token that you will provide to the experiment.service.db.ExperimentRestAPI wrapper in a python cell below.\n",
    "   - The very first time you visit the ST4SD runtime service, your browser may attempt to use stale cookies. Please visit `${route_st4sd_runtime_service}/oauth/sign_in` to trigger a new login cycle and invalidate your browser stale cookies.\n",
    "      - For example: https://st4sd-prod.ve-5446-9ca4d14d48413d18ce61b80811ba4308-0000.us-south.containers.appdomain.cloud/oauth/sign_in\n",
    "\n",
    "**The token will last for 168 hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if authentication_enabled:\n",
    "    auth_url = '/'.join((route_st4sd_runtime_service, 'authorisation/token'))\n",
    "    print(f\"Visit this URL to get your authentication token:\\n{auth_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell and paste in the widget below the value of the authorization token you've been presented with when you visited the URL in the previous cell. Alternatively, you can use an OpenShift token (the one that you'd normaly provide to the `--token` parameter of oc login) with the `cc_bearer_key` agument to experiment.service.db.ExperimentRestAPI\n",
    "\n",
    "NOTE: Visual Studio Code's renderer does not currently allow pasting in widgets such as the one below. If this is the case for you, you can fall back to pasting the token in the cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_label = 'Input your authentication token here:'\n",
    "display(w_label)\n",
    "token_widget = widgets.Password()\n",
    "display(token_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = ''\n",
    "if authentication_enabled and auth_token == '':\n",
    "    auth_token = token_widget.value\n",
    "    if auth_token == '':\n",
    "        print(\"Authentication is required. Please fill in your token in the box above.\")\n",
    "        raise Exception(\"Missing authentication\")\n",
    "    if auth_token.startswith(\"\\\"\"):\n",
    "        auth_token = auth_token[1:]\n",
    "    if auth_token.endswith(\"\\\"\"):\n",
    "        auth_token = auth_token[:-1]\n",
    "    token_widget.value = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the API\n",
    "\n",
    "The cell below will validate the token you provided, ensuring you have successfully authenticated to ST4SD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your account is authorised to use the ST4SD Runtime Service REST API\n",
    "try:\n",
    "    api = experiment.service.db.ExperimentRestAPI(route_st4sd_runtime_service, route_st4sd_registry, \n",
    "                                      route_st4sd_rest, max_retries=2, secs_between_retries=1,\n",
    "                                      cc_auth_token=auth_token)\n",
    "except experiment.service.errors.UnauthorisedRequest as e:\n",
    "    print(f\"Visit {auth_url} to retrieve your authentication token. Then use it to set the value of \"\n",
    "          \"\\\"auth_token\\\" in the above cell. Execute that cell and then execute this one.\")\n",
    "else:\n",
    "    print(f\"You've successfully authenticated to {route_st4sd_runtime_service}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List and Add Parameterised Virtual Experiment Packages\n",
    "\n",
    "Find out more about parameterised virtual experiment packages (i.e.`experiments`) on our [website](https://pages.github.ibm.com/st4sd/overview/creating-a-parameterised-package/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query available experiments\n",
    "experiments = api.api_experiment_list()\n",
    "to_show = 5\n",
    "\n",
    "print(f\"There are {len(experiments.keys())} registered experiments\", end='')\n",
    "if len(experiments.keys()) > to_show:\n",
    "    print(\". The first 5 are:\", end='\\n\\n')\n",
    "else:\n",
    "    print(\":\", end='\\n\\n')\n",
    "\n",
    "for idx, e in enumerate(experiments.keys()):\n",
    "    if idx > to_show:\n",
    "        break\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(experiments.keys()) > 0:\n",
    "    print(\"The entry for the first experiment is:\", end='\\n\\n')\n",
    "    first_experiment = experiments[list(experiments.keys())[0]]\n",
    "    print(pretty_json(first_experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This adds a band-gap-pm3-gamess-us to the target ST4SD deployment\n",
    "#If it already exists its definition gets updated to match what is typed below\n",
    "package = {\n",
    "    \"base\": {\n",
    "        \"packages\": [\n",
    "            {\n",
    "                \"source\": {\n",
    "                    \"git\": {\n",
    "                        \"location\": {\n",
    "                            \"url\": \"https://github.com/st4sd/band-gap-gamess.git\",\n",
    "                            \"tag\": \"1.0.0\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"config\": {\n",
    "                    \"path\": \"semi-empirical/homo-lumo-dft-semi-empirical.yaml\",\n",
    "                    \"manifestPath\": \"semi-empirical/manifest.yaml\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"package\": {\n",
    "            \"name\": \"band-gap-pm3-gamess-us\",\n",
    "            \"tags\": [\n",
    "                \"latest\",\n",
    "                \"1.0.0\"\n",
    "            ],\n",
    "            \"maintainer\": \"https://github.com/michael-johnston\",\n",
    "            \"description\": \"Uses the PM3 semi-empirical method to perform the geometry optimization and calculate the band-gap and related properties. The calculation is performed with GAMESS-US\",\n",
    "            \"keywords\": [\n",
    "                \"smiles\",\n",
    "                \"computational chemistry\",\n",
    "                \"semi-empirical\",\n",
    "                \"geometry-optimization\",\n",
    "                \"pm3\",\n",
    "                \"homo\",\n",
    "                \"lumo\",\n",
    "                \"band-gap\",\n",
    "                \"gamess-us\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"parameterisation\": {\n",
    "        \"presets\": {\n",
    "            \"runtime\": {\n",
    "                \"args\": [\n",
    "                    \"--failSafeDelays=no\",\n",
    "                    \"--registerWorkflow=yes\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"executionOptions\": {\n",
    "            \"variables\": [\n",
    "                {\n",
    "                    \"name\": \"numberMolecules\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"startIndex\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\":  \"gamess-walltime-minutes\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\":  \"gamess-grace-period-seconds\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\":  \"number-processors\"\n",
    "                }\n",
    "            ],\n",
    "            \"platform\": [\n",
    "                \"openshift\",\n",
    "                \"openshift-kubeflux\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "package = api.api_experiment_push(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The package you get back contains information that the registry auto-discovers about it\n",
    "# uncomment the line below if you want to take a look at this information\n",
    "# print(json.dumps(package, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may delete a version of the band-gap-pm3-gamess-us by referencing it \n",
    "# either via a `:tag` or the full `@${digest}`\n",
    "\n",
    "# For example:\n",
    "# identifier = '@'.join(package['metadata']['package']['name'], package['metadata']['registry']['digest'])\n",
    "# api.api_experiment_delete(identifier)\n",
    "\n",
    "# WARNING: The only way re-instate the experiment definition \n",
    "# is to api_experiment_push() it once again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data - A csv formatted string containing a label a SMILES\n",
    "#You can also use the next cell to load a file from your hard drive instead\n",
    "molecules = '''label,smiles\n",
    "mymolecule,CCCCCCCC[SH2+]\n",
    "'''.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read some input data for band-gap-pm3-gamess-us experiment\n",
    "# df: pandas.DataFrame = pandas.read_csv('~/my_molecules.csv', engine=\"python\", sep=None)\n",
    "# df_filtered = df[[\"label\", \"smiles\"]]\n",
    "# molecules = df_filtered.to_csv(index=False).rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input configuration\n",
    "#See the experiment description for defaults and other options\n",
    "\n",
    "# Remove stray newlines at end of string representation of CSV file\n",
    "molecules = molecules.rstrip()\n",
    "\n",
    "experimentConfiguration = {\n",
    "    \"inputs\": [\n",
    "        {\"filename\": \"input_smiles.csv\", \"content\": molecules}\n",
    "    ],\n",
    "    \"data\": [\n",
    "      # (Optional) override contents of data \n",
    "      # files, similarly to providing inputs\n",
    "      # Note: \"band-gap-pm3-gamess\" does not support overriding data files\n",
    "      # because it does not contain parameterisation.executionOptions.data[] settings\n",
    "      # e.g. if it supported overriding the data file \"input_molecule.txt\"\n",
    "      # the following Dictionary would be valid here:\n",
    "      # {\"filename\": \"input_molecule.txt\", \n",
    "      #  \"content\": \"the string representation of the file contents\"}\n",
    "    ],\n",
    "    \"variables\": {\n",
    "        \"startIndex\": 0,\n",
    "        # you can submit multiple molecules in 1 experiment\n",
    "        \"numberMolecules\": len(molecules.split(\"\\n\")) -1,\n",
    "    },\n",
    "    \"metadata\": {\n",
    "      # you can provide user-metadata `key: value` pairs which you can use\n",
    "      # in the future for querying the database (user-metadata)\n",
    "      \"author\": \"amazing-person\"\n",
    "    },\n",
    "    \"additionalOptions\": [\n",
    "      # you can provide additional arguments to elaunch here\n",
    "      # but they cannot override those of the experiment definition\n",
    "      # the additionalOptions of which will automatically be used too\n",
    "      \"--useMemoization=y\"\n",
    "    ],\n",
    "    \"orchestrator_resources\": {\n",
    "      \"cpu\": \"1\",\n",
    "      \"memory\": \"2Gi\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit an instance of the parameterised virtual experiment package\n",
    "experimentId = 'band-gap-pm3-gamess-us'\n",
    "rest_uid = api.api_experiment_start(experimentId, experimentConfiguration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print REST-uid of experiment instance\n",
    "print(\"rest-uid:\", rest_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get instance status - run this cell periodically, until experiment state becomes \"Finished\"\n",
    "#it should take about 5 minutes\n",
    "instance_status = api.api_rest_uid_status(rest_uid)\n",
    "\n",
    "status = instance_status['status']\n",
    "status = {key: status[key] for key in status if key != \"meta\"}\n",
    "print(\"Status of instance is\\n\",json.dumps(status, indent=2))\n",
    "\n",
    "#Uncomment to see verbose state of instance\n",
    "#PrettyInstanceStatus(instance_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtual experiments produce \"key-outputs\" which you may download (see cell below).\n",
    "\n",
    "The above experiment produces just 1 output `AnionResults`. The cell below will keep running until the virtual experiment has produced an output, it will then fetch it, display it, and exit the `while` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    instance_status = api.api_rest_uid_status(rest_uid)\n",
    "\n",
    "    print(f\"Outputs produced so far are {pretty_json(instance_status['outputs'])}\", end='\\n\\n')\n",
    "    exp_state = instance_status['status']['experiment-state']\n",
    "    if exp_state is None:\n",
    "        next_call = datetime.datetime.now() + datetime.timedelta(seconds=10)\n",
    "        print(f\"Kubernetes is spinning up objects - try again at {next_call}\")\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "                                         \n",
    "    if exp_state in [\"running\", \"initialising\"]:\n",
    "        print(f\"Experiment is {exp_state}, it may produce more outputs\")\n",
    "        print(\"The experiment in this example, only produces 1 output - OptimisationResults\")\n",
    "    else:\n",
    "        print(f\"Experiment is {exp_state} - it will not produce new outputs\", end='\\n\\n')\n",
    "\n",
    "    # Get the CSV data associated with a particular result type\n",
    "    # If you attempt to fetch an output for which there is no entry in the instance_status['outputs'] dictionary,\n",
    "    # or there is no workflow instance the statement below will raise an InvalidHTTPRequest exception, \n",
    "    # read the description of the Exception for more information.\n",
    "    if 'OptimisationResults' in instance_status['outputs']:\n",
    "        filename, contents = api.api_rest_uid_output(rest_uid, 'OptimisationResults')\n",
    "        print(\"Contents (i.e. bytes) of\", filename, \"are:\")\n",
    "        print(contents.decode('utf-8'))\n",
    "        break\n",
    "    else:\n",
    "        next_call = datetime.datetime.now() + datetime.timedelta(seconds=10)\n",
    "        print(f\"Experiment has not produced outputs yet - try again at {next_call}\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment instance will asynchronously register itself to the ST4SD datastore - this may take up to 1 minute after the virtual experiment instance transitions to the `running` state. It will then asynchronously update its state on the ST4SD datastore database.\n",
    "\n",
    "\n",
    "One of the features of the Datastore (often referred to as ST4SD Centralized Database or CDB) is the generation of status reports about the experiment instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First wait for the experiment to register itself to the ST4SD Datastore\n",
    "# this can take up to 1 minute after the virtual experiment instance transitions to the \n",
    "\n",
    "print(f\"Waiting for virtual experiment instance {rest_uid} to register to the ST4SD Datastore\")\n",
    "\n",
    "instance_uri = None\n",
    "while instance_uri is None:\n",
    "    doc = api.cdb_get_document_experiment_for_rest_uid(rest_uid, query={\n",
    "        \"status.experiment-state\": \"finished\"})   \n",
    "    if doc is not None:\n",
    "        instance_uri = doc['instance']\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "\n",
    "print(f\"The Instance URI of the virtual experiment is {instance_uri} - fetching the status report\")\n",
    "report = None\n",
    "\n",
    "while report is None:\n",
    "    try:\n",
    "        report = api.cdb_get_detailed_status_report(instance_uri)\n",
    "        break\n",
    "    except ValueError as e: \n",
    "        print(f\"{e} - try again in 10 seconds\")\n",
    "        time.sleep(10)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Interface of the Experiment instance\n",
    "\n",
    "Experiments may optionally have an interface that maps an input system id (e.g. a SMILES) to properties that the parameterised virtual experiment package measures. Interfaces help users understand the contents of the output of a virtual experiment without the need to understand the internals of the experiment.\n",
    "\n",
    "For more information, read our [documentation](https://st4sd.github.io/overview/using-a-virtual-experiment-interface/) about interfaces of parameterised virtual experiment packages\n",
    "\n",
    "Below, we demonstrate extracting the interface of the experiment instance you ran earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = api.cdb_get_document_experiment_for_rest_uid(rest_uid, include_properties=[\"*\"])\n",
    "df: pandas.DataFrame = pandas.DataFrame.from_dict(doc['interface']['propertyTable'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with S3 and Datasets\n",
    "\n",
    "You may use `input` and `data` files stored on S3 and Dataset objects ([with the help of Datashim](https://github.com/datashim-io/datashim)). \n",
    "\n",
    "First, you need to create a `S3` bucket and credentials to access it. If you are using IBM Cloud, you can [use our guide](https://st4sd.github.io/overview/UsingCloudObjectStore).\n",
    "\n",
    "### Using S3 directly\n",
    "\n",
    "\n",
    "Below, we show how to read `input`/`data` files stored on S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the experiment description for defaults and other options\n",
    "experimentConfiguration = {\n",
    "    \"variables\": {\n",
    "        \"startIndex\": 0,\n",
    "        \"numberMolecules\": 1,\n",
    "    },\n",
    "    \"orchestrator_resources\": {\n",
    "      \"cpu\": \"1\",\n",
    "      \"memory\": \"2Gi\"\n",
    "    },\n",
    "    \"s3\": {\n",
    "      # See guide https://pages.github.ibm.com/st4sd/overview/UsingCloudObjectStore\n",
    "      # to populate the values here. If you Datashim is running on your cluster you may use\n",
    "      # Datasets instead of typing the S3 credentials here (scroll notebook)\n",
    "      \"accessKeyID\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n",
    "      \"secretAccessKey\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n",
    "      \"endpoint\":  \"https endpoint (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n",
    "      \"bucket\": \"the name of your bucket here\"\n",
    "    },\n",
    "    \"data\": [{\n",
    "        # The contents of this data fill will be read from S3\n",
    "       \"filename\": \"/path/relative/to/S3/bucket/input_smiles.csv\"\n",
    "   }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ST4SD also supports storing key-outputs of virtual experiments on S3. Recall that experiments may optionally define key-outputs (snippet from [`sum-numbers`](https://github.com/st4sd/sum-numbers/blob/main/conf/flowir_package.yaml):\n",
    "\n",
    "```yaml\n",
    "output:\n",
    "  TotalSum:\n",
    "    data-in: \"stage2.Sum/out.stdout:copy\"\n",
    "... rest of FlowIR ...\n",
    "```\n",
    "\n",
    "For example, in the FlowIR yaml above the experiment defines a single key-output named `TotalSum`. The key-output maps to the file `out.stdout` that the `Sum` component in stage `2` generates.\n",
    "\n",
    "\n",
    "We can instruct ST4SD to upload the key-outputs to the path `/run1_output/` in bucket `my-bucket` on S3 like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the experiment description for defaults and other options\n",
    "experimentConfiguration = {\n",
    "    \"variables\": {\n",
    "        \"startIndex\": 0,\n",
    "        \"numberMolecules\": 1,\n",
    "    },\n",
    "    \"orchestrator_resources\": {\n",
    "      \"cpu\": \"1\",\n",
    "      \"memory\": \"2Gi\"\n",
    "    },\n",
    "    \"s3Store\":{\n",
    "      \"credentials\": {\n",
    "        # See guide https://pages.github.ibm.com/st4sd/overview/UsingCloudObjectStore\n",
    "        # to populate the values here. If you Datashim is running on your cluster you may use\n",
    "        # Datasets instead of typing the S3 credentials here (scroll notebook)\n",
    "        \"accessKeyID\": \"the  contents of `cos_hmac_keys.access_key_id`\",\n",
    "        \"secretAccessKey\": \"the contents of  `cos_hmac_keys.secret_access_key`\",\n",
    "        \"endpoint\": \"your endpoint prefixed with https:// (e.g. https://s3.eu-de.cloud-object-storage.appdomain.cloud)\",\n",
    "        \"bucket\": \"my-bucket\", # the name of your bucket\n",
    "      },\n",
    "      # optional - defaults to \"/\"\"\n",
    "      \"bucketPath\": \"/run1_output/\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may combine the `\"s3\"` and `\"s3Store\"` fields to both read `input`/`data` files from S3 and store key-outputs to s3.\n",
    "\n",
    "## Using Datashim to access S3\n",
    "\n",
    "[Datashim](https://github.com/datashim-io/datashim) is a Kubernetes-native framework that enables you to create `Dataset` objects which act as gateways to S3 buckets and can also be mounted as regular `PersistentVolumeClaims` in your pods. \n",
    "\n",
    "Datashim is an optional component of ST4SD, you can find installation instructions here <https://github.com/datashim-io/datashim>\n",
    "\n",
    "In ST4SD we support using Datasets to provide `input`/`data` files to virtual experiments that are stored on S3 as well as store key-outputs of virtual experiments to S3.\n",
    "\n",
    "To provide `input` and `data` files that are stored on a S3 bucket for which you have already created a Dataset object ([instructions to create Dataset](https://st4sd.github.io/overview/UsingCloudObjectStore/#datashim-method)) you can use the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the experiment description for defaults and other options\n",
    "experimentConfiguration = {\n",
    "    \"variables\": {\n",
    "        \"startIndex\": 0,\n",
    "        \"numberMolecules\": 1,\n",
    "    },\n",
    "    \"orchestrator_resources\": {\n",
    "      \"cpu\": \"1\",\n",
    "      \"memory\": \"2Gi\"\n",
    "    },\n",
    "    \"s3\": {\n",
    "      # By specifying a `dataset` we do not need to type out the credentials for accessing S3\n",
    "      \"dataset\": \"my-dataset\"\n",
    "    },\n",
    "    \"data\": [{\n",
    "        # The contents of this data fill will be read from the S3 bucket that `my-dataset` proxies\n",
    "       \"filename\": \"/path/relative/to/dataset/bucket/input_smiles.csv\"\n",
    "   }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instruct ST4SD to upload the key-outputs to the path `/run1_output/` in the bucket `my-bucket` that the `my-dataset` proxies so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the experiment description for defaults and other options\n",
    "experimentConfiguration = {\n",
    "    \"variables\": {\n",
    "        \"startIndex\": 0,\n",
    "        \"numberMolecules\": 1,\n",
    "    },\n",
    "    \"orchestrator_resources\": {\n",
    "      \"cpu\": \"1\",\n",
    "      \"memory\": \"2Gi\"\n",
    "    },\n",
    "    \"s3Store\": {\n",
    "      \"datasetStoreURI\": \"dataset://my-dataset/run1_output/\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you may use both `s3.dataset` and `s3Store.datasetStoreURI` to read `input`/`data` from a dataset and store them on a dataset (the same or a different one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mounting Kubernetes objects (Secret, PersistentVolumeClaim, Dataset, ConfigMap)\n",
    "\n",
    "You may mount `readOnly` volumes to your virtual-experiments and map them to `application-dependencies` which are folders that appear in the root-directory of your virtual experiment instances. You can reference similar to how you reference the files under the `data` and `input` top-level directories.\n",
    "\n",
    "The schema of the `volumes` array is:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"volumes\": [\n",
    "      {\n",
    "        \"type\":\n",
    "          {\n",
    "            # You must use exactly one of the fields below\n",
    "            \"configMap\": \"<name of configMap, OR\",\n",
    "            \"persistentVolumeClaim\": \"<name of PVC>, OR\",\n",
    "            \"dataset\": \"name of Dataset\",\n",
    "            \"secret\": \"name of Secret\",\n",
    "          },\n",
    "         \"applicationDependency\": \"application-dependencies entry that will point to contents of volume\",\n",
    "         \"subPath\": \"Path within the volume from which the container's volume should be mounted. Defaults to empty string (volume's root) - not applicable to configMaps or secrets\"\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "We can mount a `Dataset` as the `foo` application dependency of an virtual experiment.\n",
    "\n",
    "First, we need to include the application dependency in the FlowIR definition of the virtual experiment:\n",
    "\n",
    "```yaml\n",
    "application-dependencies:\n",
    "  default:\n",
    "  # indicate that this workflow expects a `foo` dependency.\n",
    "  - foo\n",
    " \n",
    "# Components reference the contents of the `foo` in the same way \n",
    "# that they reference data/input files, for example:\n",
    "components:\n",
    "- name: hello\n",
    "  command:\n",
    "    executable: cat\n",
    "    arguments: foo/message.txt:ref\n",
    "  references:\n",
    "  - foo/message.txt:ref\n",
    "```\n",
    "\n",
    "We can use the payload below to mount the S3 bucket `my-bucket` that the `my-dataset` Dataset proxies as the `foo` application-dependency like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the experiment description for defaults and other options\n",
    "experimentConfiguration = {\n",
    "    \"variables\": {\n",
    "        \"startIndex\": 0,\n",
    "        \"numberMolecules\": 1,\n",
    "    },\n",
    "    \"orchestrator_resources\": {\n",
    "      \"cpu\": \"1\",\n",
    "      \"memory\": \"2Gi\"\n",
    "    },\n",
    "    \"s3Store\": {\n",
    "      \"datasetStoreURI\": \"dataset://my-dataset/run1_output/\"\n",
    "    },\n",
    "    \"volumes\":[\n",
    "        {\n",
    "            \"type\": {\n",
    "                \"dataset\": \"my-dataset\"\n",
    "            },\n",
    "            \"applicationDependency\": \"foo\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the ST4SD Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the `st4sd-datastore` API will truncate files that are larger than 32MB. In such a case\n",
    "the returned contents will include the message below right at the end of the truncated contents:\n",
    "`FILE TRUNCATED to 33554432 bytes, actual file size is <the-file-size-in-bytes>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = api.cdb_get_document_experiment(query={})\n",
    "print(f\"Recorded workflow instances: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print each `experiment` document\n",
    "# for d in docs:\n",
    "#     print(pretty_json(d['type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs, files = api.cdb_get_data(stage=1, instance='band-gap-.*', component='ExtractEnergies', filename='energies.csv')\n",
    "print(\"Last Matching component document is\", end='\\n\\n')\n",
    "print(pretty_json(docs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pull Secrets\n",
    "\n",
    "Kubernetes uses so-called imagePullSecrets to pull images from container registries. If your workflow instances need to access a container registry use the API below to create/update imagePullSecrets. You may also list the existing imagePullSecrets that the mvp2-stack utilizes to pull images for the pods your workflow instances generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(experiment.service.db.ExperimentRestAPI.api_image_pull_secrets_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(experiment.service.db.ExperimentRestAPI.api_image_pull_secrets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "If your cluster-admin has installed [Datashim](https://github.com/datashim-io/datashim) (formerly known as DLF) you will be able to create and mount Datasets using the Datashim framework. For the time being, Datashim supports Dataset objects which point to COS/S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(experiment.service.db.ExperimentRestAPI.api_datasets_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(experiment.service.db.ExperimentRestAPI.api_datasets_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
